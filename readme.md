# Awesome Detection Transformer
This a collecttion of papers for detection and segmentation with Transformer .
</br>
If you find some overlooked papers or resourses, please open issues or pull requests (recommended).

# Papers
## 2022
<p>
<b>DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection.</b> 
<br>
<font size=2>Feng Li*, Hao Zhang*, Shilong Liu*, Lei Zhang, Hang Su, Jun Zhu, Lionel M. Ni, Heung-Yeung Shum</font>
<br>
arxiv 2022.
<a href='https://arxiv.org/abs/2203.03605'>[paper]</a> <a href='https://github.com/IDEACVR/DINO'>[code]</a>  
</p>

<p>
<font size=3><b>DN-DETR: Accelerate DETR Training by Introducing Query DeNoising.</b></font>
<br>
<font size=2>Feng Li*, Hao Zhang*, Shilong Liu, Jian Guo, Lionel M. Ni, Lei Zhang.</font>
<br>
<font size=2>CVPR 2022.</font>
<a href='https://arxiv.org/abs/2203.01305'>[paper]</a> <a href='https://github.com/FengLi-ust/DN-DETR'>[code]</a>    
</p>

<p>
<font size=3><b>DAB-DETR: Dynamic Anchor Boxes are Better Queries for DETR.</b></font>
<br>
<font size=2>Shilong Liu, Feng Li, Hao Zhang, Xiao Yang, Xianbiao Qi, Hang Su, Jun Zhu, Lei Zhang.</font>
<br>
<font size=2>ICLR 2022.</font>
<a href='https://arxiv.org/abs/2201.12329'>[paper]</a> <a href='https://github.com/SlongLiu/DAB-DETR'>[code]</a>    
</p>

<p>
<b>D^2ETR: Decoder-Only DETR with Computationally Efficient Cross-Scale Attention.</b> 
<br>
<font size=2>Junyu Lin, Xiaofeng Mao, Yuefeng Chen, Lei Xu, Yuan He, Hui Xue</font>
<br>
arxiv 2022.
<a href='https://arxiv.org/abs/2203.00860'>[paper]</a>  
</p>


<p>
<font size=3><b>Sparse DETR: Efficient End-to-End Object Detection with Learnable Sparsity.</b></font>
<br>
<font size=2>Byungseok Roh, JaeWoong Shin, Wuhyun Shin, Saehoon Kim.</font>
<br>
<font size=2>ICLR 2022.</font>
<a href='https://arxiv.org/abs/2111.14330v2'>[paper]</a> <a href='https://github.com/kakaobrain/sparse-detr'>[code]</a>    
</p>

<p>
<font size=3><b>Anchor DETR: Query Design for Transformer-Based Object Detection.</b></font>
<br>
<font size=2>Yingming Wang, Xiangyu Zhang, Tong Yang, Jian Sun.</font>
<br>
<font size=2>AAAI 2022.</font>
<a href='https://arxiv.org/abs/2109.07107v2'>[paper]</a> <a href='https://github.com/megvii-research/AnchorDETR'>[code]</a>    
</p>




## 2021
<p>
<font size=3><b>[Mask2Former] Masked-attention Mask Transformer for Universal Image Segmentation .</b></font>
<br>
<font size=2>Bowen Cheng, Ishan Misra, Alexander G. Schwing, Alexander Kirillov, Rohit Girdhar.</font>
<br>
<font size=2>arxiv 2021.</font>
<a href='https://arxiv.org/abs/2112.01527'>[paper]</a> <a href='https://github.com/facebookresearch/Mask2Former'>[code]</a>    
</p>

<p>
<font size=3><b>[MaskFormer] Per-Pixel Classification is Not All You Need for Semantic Segmentation.</b></font>
<br>
<font size=2>Bowen Cheng, Alexander G. Schwing, Alexander Kirillov.</font>
<br>
<font size=2>NeurIPS 2021.</font>
<a href='https://arxiv.org/abs/2107.06278'>[paper]</a> <a href='https://github.com/facebookresearch/MaskFormer'>[code]</a>    
</p>


<p>
<font size=3><b>Dynamic DETR: End-to-End Object Detection With Dynamic Attention.</b></font>
<br>
<font size=2>Xiyang Dai, Yinpeng Chen, Jianwei Yang, Pengchuan Zhang, Lu Yuan, Lei Zhang.</font>
<br>
<font size=2>ICCV 2021.</font>
<a href='https://openaccess.thecvf.com/content/ICCV2021/papers/Dai_Dynamic_DETR_End-to-End_Object_Detection_With_Dynamic_Attention_ICCV_2021_paper.pdf'>[paper]</a> 
<!-- <a href='https://github.com/atten4vis/conditionaldetr'>[code]</a>     -->
</p>

<p>
<font size=3><b>Conditional DETR for Fast Training Convergence.</b></font>
<br>
<font size=2>Yingming Wang, Xiangyu Zhang, Tong Yang, Jian Sun.</font>
<br>
<font size=2>ICCV 2021.</font>
<a href='https://arxiv.org/abs/2108.06152v2'>[paper]</a> <a href='https://github.com/atten4vis/conditionaldetr'>[code]</a>    
</p>

<p>
<font size=3><b>Rethinking Transformer-based Set Prediction for Object Detection.</b></font>
<br>
<font size=2>Zhiqing Sun, Shengcao Cao, Yiming Yang, Kris Kitani.</font>
<br>
<font size=2>ICCV 2021.</font>
<a href='https://arxiv.org/abs/2011.10881'>[paper]</a> <a href='https://github.com/edward-sun/tsp-detection'>[code]</a>    
</p>

<p>
<font size=3><b>Fast Convergence of DETR with Spatially Modulated Co-Attention.</b></font>
<br>
<font size=2>Peng Gao, Minghang Zheng, Xiaogang Wang, Jifeng Dai, Hongsheng Li .</font>
<br>
<font size=2>ICCV 2021.</font>
<a href='https://arxiv.org/abs/2010.04159v4'>[paper]</a> <a href='https://github.com/gaopengcuhk/SMCA-DETR'>[code]</a>    
</p>

<p>
<font size=3><b>MDETR -- Modulated Detection for End-to-End Multi-Modal Understanding.</b></font>
<br>
<font size=2>Aishwarya Kamath, Mannat Singh, Yann LeCun, Gabriel Synnaeve, Ishan Misra, Nicolas Carion.</font>
<br>
<font size=2>ICCV 2021.</font>
<a href='https://arxiv.org/abs/2104.12763'>[paper]</a> <a href='https://github.com/ashkamath/mdetr'>[code]</a>    
</p>

<p>
<font size=3><b>Efficient DETR: Improving End-to-End Object Detector with Dense Prior.</b></font>
<br>
<font size=2>Zhuyu Yao, Jiangbo Ai, Boxun Li, Chi Zhang.</font>
<br>
<font size=2>srxiv 2021.</font>
<a href='https://arxiv.org/abs/2104.01318'>[paper]</a>
<!-- <a href='https://github.com/dddzg/up-detr'>[code]</a>     -->
</p>

<p>
<font size=3><b>UP-DETR: Unsupervised Pre-training for Object Detection with Transformers.</b></font>
<br>
<font size=2>Zhigang Dai, Bolun Cai, Yugeng Lin, Junying Chen.</font>
<br>
<font size=2>CVPR 2021.</font>
<a href='https://arxiv.org/abs/2011.09094'>[paper]</a> <a href='https://github.com/dddzg/up-detr'>[code]</a>    
</p>

<p>
<font size=3><b>Deformable DETR: Deformable Transformers for End-to-End Object Detection.</b></font>
<br>
<font size=2>Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, Jifeng Dai.</font>
<br>
<font size=2>ICLR 2021.</font>
<a href='https://arxiv.org/abs/2010.04159v4'>[paper]</a> <a href='https://github.com/fundamentalvision/Deformable-DETR'>[code]</a>    
</p>

## 2020
<p>
<font size=3><b>[DETR] End-to-End Object Detection with Transformers.</b></font>
<br>
<font size=2>Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, Sergey Zagoruyko.</font>
<br>
<font size=2>ECCV 2020.</font>
<a href='https://arxiv.org/abs/2005.12872'>[paper]</a> <a href='https://github.com/facebookresearch/detr'>[code]</a>    
</p>

# Acknowledgements
We thank all the authors above for their great works!